{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [IAPR][iapr]: Lab 3 â€’  Classification\n",
    "\n",
    "\n",
    "**Group ID:** 37\n",
    "\n",
    "**Author 1 (sciper):** Michael Biselx    (283812)  \n",
    "**Author 2 (sciper):** Bastien Darbellay (288406)   \n",
    "**Author 3 (sciper):** Maria   Guerraoui (274578)  \n",
    "\n",
    "\n",
    "**Release date:** 07.04.2022  \n",
    "**Due date:** 25.04.2022\n",
    "\n",
    "\n",
    "## Important notes\n",
    "\n",
    "The lab assignments are designed to teach practical implementation of the topics presented during class well as\n",
    "preparation for the final project, which is a practical project which ties together the topics of the course.\n",
    "\n",
    "As such, in the lab assignments/final project, unless otherwise specified, you may, if you choose, use external\n",
    "functions from image processing/ML libraries like opencv and sklearn as long as there is sufficient explanation\n",
    "in the lab report. For example, you do not need to implement your own edge detector, etc.\n",
    "\n",
    "**! Before handling back the notebook <font color='red'> rerun </font>the notebook from scratch !**\n",
    "`Kernel` > `Restart & Run All`\n",
    "\n",
    "We will not rerun the notebook for you.\n",
    "\n",
    "\n",
    "[iapr]: https://github.com/LTS5/iapr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract relevant data\n",
    "We first need to extract the `lab-03-data.tar.gz` archive.\n",
    "To this end, we use the [tarfile] module from the Python standard library.\n",
    "\n",
    "[tarfile]: https://docs.python.org/3.6/library/tarfile.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "\n",
    "data_base_path = os.path.join(os.pardir, 'data')\n",
    "data_folder = 'lab-03-data'\n",
    "tar_path = os.path.join(data_base_path, data_folder + '.tar.gz')\n",
    "with tarfile.open(tar_path, mode='r:gz') as tar:\n",
    "    tar.extractall(path=data_base_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1\n",
    "In this part, we will study classification based on the data available in the Matlab file `classification.mat` that you will under `lab-03-data/part1`.\n",
    "There are 3 data sets in this file, each one being a training set for a given class.\n",
    "They are contained in variables `a`, `b` and `c`.\n",
    "\n",
    "**Note**: we can load Matlab files using the [scipy.io] module.\n",
    "\n",
    "[scipy.io]: https://docs.scipy.org/doc/scipy/reference/io.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "\n",
    "data_part1_path = os.path.join(data_base_path, data_folder, 'part1', 'classification.mat')\n",
    "matfile = scipy.io.loadmat(data_part1_path)\n",
    "a = matfile['a']\n",
    "b = matfile['b']\n",
    "c = matfile['c']\n",
    "\n",
    "print(a.shape, b.shape, c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Bayes method (5 pts)\n",
    "Using the Bayes method, give the expression of the separation curves between those three classes.\n",
    "Do reasonable hypotheses about the distributions of those classes and estimate the corresponding parameters based on the given training sets.\n",
    "Draw those curves on a plot, together with the training data. Note that you do not need to solve the analytical expression. You can simply evaluate the function for each point on the plane to end up with a segmentation map.\n",
    "For simplicity reasons, round the estimated parameters to the closest integer value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation of the data (since it is low-dimensional data this is possible)\n",
    "import numpy as np\n",
    "from   scipy.stats import multivariate_normal\n",
    "import matplotlib.colors\n",
    "import matplotlib.pyplot as plt\n",
    "from   matplotlib.patches import Ellipse\n",
    "%matplotlib inline\n",
    "\n",
    "def visualize_data(datas, lbls=[], stats=[], ax=[], cmap=[]): \n",
    "    while len(lbls)  < len(datas) : lbls.append([])\n",
    "    while len(stats) < len(datas) : stats.append([])  \n",
    "    if ax==[]   : fig, ax = plt.subplots(1,1, figsize=(10, 5))\n",
    "    if cmap==[] : cmap = matplotlib.colors.ListedColormap(['r', 'g', 'b', 'y'])\n",
    "        \n",
    "    for data, lbl, stat, c in zip(datas, lbls, stats, range(len(lbls))) : \n",
    "        \n",
    "        # plot scattering of data\n",
    "        ax.scatter(data[:,0], data[:,1], s=1, color=cmap(c), label=lbl)\n",
    "        \n",
    "        # if statistics on the data are available\n",
    "        if len(stat) : \n",
    "            # plot the mean\n",
    "            ax.scatter(stat[0][0], stat[0][1], color=cmap(c))\n",
    "            \n",
    "            # plot the covariance \n",
    "            [v, w] = np.linalg.eig(stat[1])\n",
    "            x = sorted(zip(v,w), reverse=True)\n",
    "            v = [v for v,w in x]           # eigenvalues give the standard deviation\n",
    "            w = x[0][1]                    # largest eigenvector gives the rotation of the semimajor axis\n",
    "            semimaj = np.sqrt(5.991*v[0])  # 95% prdicition semimajor axis\n",
    "            semimin = np.sqrt(5.991*v[1])  # 95% prdicition semiminor axis\n",
    "            angle   = np.rad2deg(np.arctan2(w[1], w[0]))\n",
    "            ellipse = Ellipse(stat[0], width =2*semimaj, height=2*semimin, angle=angle,\n",
    "                              alpha=.2, facecolor=cmap(c))\n",
    "            ax.add_artist(ellipse)\n",
    "            \n",
    "    ax.set_title(\"data visualisation\")\n",
    "    ax.legend()\n",
    "\n",
    "    \n",
    "def get_stats(data) : # get the statistics for a class of data\n",
    "    mean = np.mean(data, axis=0)\n",
    "    cov  = np.cov(data, rowvar=False)\n",
    "    return mean, cov   \n",
    "    \n",
    "\n",
    "def partition_space_Bayes(priors, stats, extent, res=0.1) : \n",
    "    X,Y  = np.mgrid[extent[0]:extent[1]:res, extent[2]:extent[3]:res]\n",
    "    grid = np.dstack((X, Y))\n",
    "    \n",
    "    # we assume the distributions are normal (gaussian) \n",
    "    # and get the probablity of finding a point of each class in any point of the grid\n",
    "    grid_probs = [prior * multivariate_normal(stat[0], stat[1]).pdf(grid) for prior, stat in zip(priors, stats)]\n",
    "\n",
    "    partition = np.argmax(grid_probs, axis=0).astype(int) # get the class with the maximum likelyhood for each gridpoint wins\n",
    "    \n",
    "    return partition, grid_probs;\n",
    "   \n",
    "    \n",
    "def get_misclassified(data, label, grid, extent) : # extract the datapoints which are misclassified by a partitioning\n",
    "    data_idx_x = (grid.shape[0] * (data[:,0] - extent[0]) / (extent[1] - extent[0])).astype(int); # rescale data to grid indices\n",
    "    data_idx_y = (grid.shape[1] * (data[:,1] - extent[2]) / (extent[3] - extent[2])).astype(int); # rescale data to grid indices\n",
    "    misclassified = data[(grid[data_idx_x, data_idx_y] != label),:]\n",
    "    \n",
    "    return misclassified\n",
    "    \n",
    "    \n",
    "def draw_misclassified(datas, labels, grid, extent, ax=[]) :  # draw the misclassified datapoints\n",
    "    if ax==[] : fig, ax = plt.subplots(1,1, figsize=(10, 5))\n",
    "        \n",
    "    misclassified = np.vstack([get_misclassified(data, label, grid, extent) for data,label in zip(data, labels)])\n",
    "    ax.scatter(misclassified[:,0], misclassified[:,1], s=10, color='white', label=\"misclassified\")\n",
    "\n",
    "    \n",
    "def get_unique_points(A, B) : # get the difference in the 2D point sets A and B\n",
    "    U = set(map(tuple, np.unique(np.vstack([A, B]), axis=0))) # set of all unique points in A and B\n",
    "    \n",
    "    A_u = np.array(list(U - set(map(tuple, B)))) # the points in A not in B\n",
    "    B_u = np.array(list(U - set(map(tuple, A)))) # the points in B not in A\n",
    "    \n",
    "    return A_u, B_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# prepare the data, labels, etc.. \n",
    "data   = [a,b,c]\n",
    "labels = ['a', 'b', 'c']\n",
    "cmap   = matplotlib.colors.ListedColormap(['r', 'g', 'b']) # colormap for coherent plotting\n",
    "stats  = [get_stats(d) for d in data] # get statistics on the distribution of the data\n",
    "priors = [d.shape[0] for d in data]   # get information on the prior distribution of the classes \n",
    "priors = priors / np.sum(priors)      # (it should be equal, since there are 200 points in each, but for cleanliness, we do it anyway)\n",
    "\n",
    "\n",
    "#visualize it for fun\n",
    "visualize_data(data, labels, stats, cmap=cmap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "extent = [-15, 15, -6, 7]\n",
    "\n",
    "# do the thing \n",
    "partition, probs = partition_space_Bayes(priors, stats, extent) # get the Bayes partitioning\n",
    "P = probs[0] + probs[1] + probs[2] # make a single probability distribution containing all points\n",
    "\n",
    "# get the number of misclassified points \n",
    "misclassifieds = [get_misclassified(d, label, partition, extent) for d, label in zip(data, [0,1,2])] \n",
    "print(\"number of misclassified points by class : \" + str([len(m) for m in misclassifieds]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# plot the results \n",
    "fig, axes = plt.subplots(2, 1, figsize=(10, 10))\n",
    "\n",
    "axes[0].imshow(P.T, origin='lower', extent=extent, cmap='gray')\n",
    "draw_misclassified(data,  labels=[0,1,2], grid=partition, extent=extent, ax=axes[0])\n",
    "visualize_data(data, labels, stats, axes[0], cmap=cmap)\n",
    "\n",
    "axes[1].imshow(partition.T, origin='lower', extent=extent, cmap=cmap, vmin=0, vmax=len(data)-1)\n",
    "draw_misclassified(data, labels=[0,1,2], grid=partition, extent=extent, ax=axes[1])\n",
    "for d in data: axes[1].scatter(d[:,0], d[:,1], s=1, color='k')\n",
    "axes[1].set_title(\"partionioned space\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Mahalanobis distance (5 pts)\n",
    "For classes `a` and `b`, give the expression of the Mahalanobis distance used to classify a point in class `a` or `b`, and verify the obtained classification, in comparison with the \"complete\" Bayes classification, for a few points of the plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your implementation and discussion\n",
    "\n",
    "def get_Mahalanobis_dist(p, stats=[], data=[]) : # expects a column-vector containing points \n",
    "    if stats == [] : stats = get_stats(data)\n",
    "    dd = p-stats[0]  \n",
    "    return (dd @ np.linalg.inv(stats[1]) @ dd.T).diagonal() # get the Mahalonobis distance \n",
    "    \n",
    "def partition_space_Mahalanobis(priors, stats, extent) :\n",
    "    X,Y  = np.mgrid[extent[0]:extent[1]:0.1, extent[2]:extent[3]:0.1]\n",
    "    grid = np.dstack((X, Y))\n",
    "    \n",
    "    # get the M. distance at each gridpoint for each class USING A DOUBLE NESTED FOR LOOP\n",
    "    grid_dists = [np.array([get_Mahalanobis_dist(p, stat) for p in grid]) for prior, stat in zip(priors, stats)]\n",
    "    \n",
    "    partition = np.argmin(grid_dists, axis=0) # the class with the minimum M. distance at each gridpoint wins\n",
    "    \n",
    "    return partition, grid_dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "extent = [-11, 15, -3, 7]\n",
    "\n",
    "# do the thing \n",
    "partition_b, gprob = partition_space_Bayes(priors[0:2], stats[0:2], extent)   # get the Bayes partitioning\n",
    "prob_b = gprob[0] + gprob[1]\n",
    "partition_m, gd = partition_space_Mahalanobis(priors[0:2], stats[0:2], extent)# get the Mahalanobis partitioning\n",
    "dm_inv = 1/gd[0] + 1/gd[1]\n",
    "\n",
    "# compare the two \n",
    "miscl_b = np.vstack([get_misclassified(d, label, partition_b, extent) for d, label in zip(data[0:2], [0,1])])\n",
    "miscl_m = np.vstack([get_misclassified(d, label, partition_m, extent) for d, label in zip(data[0:2], [0,1])])\n",
    "u_miscl = np.unique(np.vstack([miscl_b, miscl_m]), axis=0)\n",
    "u_miscl_b, u_miscl_m = get_unique_points(miscl_b, miscl_m)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# visualize \n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 10))\n",
    "\n",
    "# the data\n",
    "axes[0,0].imshow(dm_inv.T, origin='lower', extent=extent, cmap='gray')\n",
    "visualize_data(data[0:2], labels[0:2], stats[0:2], axes[0,0], cmap=cmap)\n",
    "\n",
    "# the Bayes partitioning \n",
    "axes[0,1].imshow(partition_b.T, origin='lower', extent=extent, cmap=cmap, vmin=0, vmax=len(data)-1)\n",
    "draw_misclassified(data[0:2], [0,1], partition_b, extent, ax=axes[0,1])\n",
    "for d in data[0:2]: axes[0,1].scatter(d[:,0], d[:,1], s=1, color='k')\n",
    "axes[0,1].set_title(\"partionioned space using full Bayes\")\n",
    "\n",
    "# the M. partitioning \n",
    "axes[1,1].imshow(partition_m.T, origin='lower', extent=extent, cmap=cmap, vmin=0, vmax=len(data)-1)\n",
    "draw_misclassified(data[0:2], [0,1], partition_m, extent, ax=axes[1,1])\n",
    "for d in data[0:2]: axes[1,1].scatter(d[:,0], d[:,1], s=1, color='k')\n",
    "axes[1,1].set_title(\"partionioned space using Mahalanobis\")\n",
    "\n",
    "# plot the differences\n",
    "axes[1,0].imshow((partition_b-partition_m).T,  origin='lower', extent=extent, cmap=matplotlib.colors.ListedColormap([ 'r', 'k', 'g']), vmin=-1, vmax=1)\n",
    "axes[1,0].scatter(u_miscl_b[:,0], u_miscl_b[:,1], label=\"miscl. by B. but not M.\")\n",
    "axes[1,0].scatter(u_miscl_m[:,0], u_miscl_m[:,1], label=\"miscl. by M. but not B.\")\n",
    "axes[1,0].scatter(u_miscl[:,0], u_miscl[:,1], s=4, color=\"white\", label=\"all misclassifieds\")\n",
    "axes[1,0].set_title(\"difference\")\n",
    "axes[1,0].legend()\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2\n",
    "In this part, we aim to classify digits using the complete version of MNIST digits dataset.\n",
    "The dataset consists of 60'000 training images and 10'000 test images of handwritten digits.\n",
    "Each image has size 28x28, and has assigned a label from zero to nine, denoting the digits value.\n",
    "Given this data, your task is to construct a Multilayer Perceptron (MLP) for supervised training and classification and evaluate it on the test images.\n",
    "\n",
    "Download the MNIST dataset (all 4 files) from http://yann.lecun.com/exdb/mnist/ under `lab-03-data/part2`.\n",
    "You can then use the script provided below to extract and load training and testing images in Python. \n",
    "\n",
    "**! Warning**: When the lab was created the official MNIST repo was down, if it is still the case please use https://github.com/mkolod/MNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Dataset loading\n",
    "Here we first declare the methods `extract_data` and `extract_labels` so that we can reuse them later in the code.\n",
    "Then we extract both the data and corresponding labels, and plot randomly some images and corresponding labels of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def extract_data(filename, image_shape, image_number):\n",
    "    with gzip.open(filename) as bytestream:\n",
    "        bytestream.read(16)\n",
    "        buf = bytestream.read(np.prod(image_shape) * image_number)\n",
    "        data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
    "        data = data.reshape(image_number, image_shape[0], image_shape[1])\n",
    "    return data\n",
    "\n",
    "\n",
    "def extract_labels(filename, image_number):\n",
    "    with gzip.open(filename) as bytestream:\n",
    "        bytestream.read(8)\n",
    "        buf = bytestream.read(1 * image_number)\n",
    "        labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_shape = (28, 28)\n",
    "train_set_size = 60000\n",
    "test_set_size = 10000\n",
    "\n",
    "data_part2_folder = os.path.join(data_base_path, data_folder, 'part2')\n",
    "\n",
    "train_images_path = os.path.join(data_part2_folder, 'train-images-idx3-ubyte.gz')\n",
    "train_labels_path = os.path.join(data_part2_folder, 'train-labels-idx1-ubyte.gz')\n",
    "test_images_path = os.path.join(data_part2_folder, 't10k-images-idx3-ubyte.gz')\n",
    "test_labels_path = os.path.join(data_part2_folder, 't10k-labels-idx1-ubyte.gz')\n",
    "\n",
    "train_images = extract_data(train_images_path, image_shape, train_set_size)\n",
    "test_images = extract_data(test_images_path, image_shape, test_set_size)\n",
    "train_labels = extract_labels(train_labels_path, train_set_size)\n",
    "test_labels = extract_labels(test_labels_path, test_set_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prng = np.random.RandomState(seed=123456789)  # seed to always re-draw the same distribution\n",
    "plt_ind = prng.randint(low=0, high=train_set_size, size=10)\n",
    "\n",
    "fig, axes = plt.subplots(1, 10, figsize=(12, 3))\n",
    "for ax, im, lb in zip(axes, train_images[plt_ind], train_labels[plt_ind]):\n",
    "    ax.imshow(im, cmap='gray')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(lb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 MLP (10 pts)\n",
    "\n",
    "To create an MLP you are free to choose any library.\n",
    "In case you don't have any preferences, we encourage you to use the [scikit-learn] package; it is a simple, efficient and free tool for data analysis and machine learning.\n",
    "In this [link][sklearn-example], you can find a basic example to see how to create and train an MLP using [scikit-learn].\n",
    "Your network should have the following properties:\n",
    "* Input `x`: 784-dimensional (i.e. 784 visible units representing the flattened 28x28 pixel images).\n",
    "* 100 hidden units `h`.\n",
    "* 10 output units `y`, i.e. the labels, with a value close to one in the i-th class representing a high probability of the input representing the digit `i`.\n",
    "\n",
    "If you need additional examples you can borrow some code from image classification tutorials.\n",
    "However, we recommend that you construct a minimal version of the network on your own to gain better insights.\n",
    "\n",
    "[scikit-learn]: http://scikit-learn.org/stable/index.html\n",
    "[sklearn-example]: http://scikit-learn.org/stable/modules/neural_networks_supervised.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your implementation and discussion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
